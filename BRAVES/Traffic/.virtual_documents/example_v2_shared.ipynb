








# Importings
import geopandas as gpd
from datetime import datetime
from trafficdata import TrafficServer
from trafficdata.core.package import get_traffic_dataset
from pathlib import Path
from pandas import Grouper
import pandas as pd
from trafficdata.core.trafficmeasurement import replace_min_plateau_using_gauss





# Data server paths
traffic_server_url = 'http://150.162.52.2:6140'              # Tomtom
reference_server_url = 'http://150.162.52.2:6150/2025_5_12'  # Geofabrik





# Importing data mask example
mask = gpd.read_file('./inputs/floripa_sc.gpkg')
# mask['geometry'] = mask.unary_union.envelope
mask.explore()





# Loading traffic server
ts = TrafficServer(
    traffic_server_url,
    reference_server_url
)

_ = ts.sync()





# List available datetime (UTM)
ts.list_available_dates()





# Defining dates
start_date = datetime(2025,7,1,0)
end_date = datetime(2025,7,1,23)

print(f'Tempo a ser calculado: {end_date - start_date}')





from datetime import timedelta

# Creating TrafficData object
td = get_traffic_dataset(
    mask,
    start_date,
    # datetime(2025,6,22,22),
    end_date,
    ts,
    n_processes=4,
    verbose=True,
    clip_by_mask=True,
    buffer_size=5,
    join_factor=0.25,
    use_hd=True
)








import geopandas as gpd
import dask.dataframe as dd
grande_flp = gpd.read_file('./inputs/GrandeFlorianopolis.gpkg')
mun_gcr = grande_flp[grande_flp['CD_MUN'].isin(['4216602', '4202305'])]


grande_flp.head(20)


mun_union = mun_gcr.unary_union
mun_union = gpd.GeoSeries([mun_union], crs=grande_flp.crs)
mun_union.to_file('./inputs/saojose_biguacu.gpkg', driver="GPKG")


intersect_df = gpd.sjoin(td.get_data(), mun_gcr[['CD_MUN', 'geometry']], how='inner', predicate='intersects')


recortado = gpd.clip(td.get_data(), mun_gcr)


from trafficdata import TrafficData  # importa a classe

# recortado é um GeoDataFrame com as ruas dentro do município
municipio_td = TrafficData(recortado)


type(municipio_td)


..


# Plotting map
traffic_example = td.loc[td.datetime == '2025-07-1 22:00:00']

traffic_example['v_inst_per_v_freeflow'] = traffic_example.eval(
    '1 - (traffic_level / inferred_freeflow_speed)'
).astype('float')

traffic_example.explore(
    column='v_inst_per_v_freeflow',
    cmap='autumn_r',
    tiles='CartoDB dark_matter')








# Definindo id da estrada
road_id = 35893655

# Obtendo dados agrupados por dia, numa média horária
daily_group = td.count_vehicles(
    road_id
)

daily_group.rename_axis(['weekday', 'hour'], inplace=True)

daily_group





# selecionar São José
mun_sj = grande_flp[grande_flp['CD_MUN'].isin(['4216602'])]

# transformar dados em GeoDataFrame
td_gdf = gpd.GeoDataFrame(td.copy(), geometry='geometry', crs=mun_sj.crs)

# interseção espacial (só com GeoDataFrame completo, não a Series geometry)
intersect_df = gpd.sjoin(td_gdf, mun_sj[['CD_MUN', 'geometry']],
                         how='inner', predicate='intersects')

# recorte exato
recortado = gpd.clip(intersect_df, mun_sj)

# recortado é um GeoDataFrame com as ruas dentro do município
municipio_td = TrafficData(recortado)




# Definindo id da estrada
#road_id = 35893655

# Obtendo dados agrupados por dia, numa média horária
daily_group = td.count_vehicles(
    # road_id,
    grouper=Grouper(
        key='datetime',
        freq='1H'  # Daily frequency
    )
)

vehicle_count_df_weekly = replace_min_plateau_using_gauss(
    daily_group,
    sigma=1,
    set_plateau_to_min=True
    )

# Corrigindo datetime
vehicle_count_df_weekly['datetime'] = vehicle_count_df_weekly['datetime'] - pd.Timedelta(hours=3)
vehicle_count_df_weekly

# Multiplicando pelas 24 horas para obter o total diário
# daily_group['average_daily_vehicle_count'] = daily_group['vehicle_count'] * 24
vehicle_count_df_weekly['road_length'] = vehicle_count_df_weekly.osm_id.apply(lambda x: td.get_road_length(x).slice_length)  # td.get_road_length(road_id)
vehicle_count_df_weekly['vkt_per_hour'] = (
    vehicle_count_df_weekly['vehicle_count'] * vehicle_count_df_weekly['road_length']
)

# Adicionando outras informações relevantes
# daily_group['surface'] = td.loc[td.osm_id == road_id].surface.iloc[0]
# daily_group['avg_traffic_level'] = td.get_values_aggregated_by_road_id(
#     road_id, 'traffic_level', grouper=Grouper(key='datetime', freq='1D')
#     )['traffic_level']
# daily_group_geom, daily_group_geom_crs = td.get_road_geometry(road_id)
# daily_group['geometry'] = daily_group_geom


# Adding metadata information
vehicle_count_df_weekly.attrs = {
    'vehicle_count': 'Average hourly vehicle count [vehicles / hour]',
    'average_daily_vehicle_count': 'Average daily vehicle count [vehicles / day]',
    'road_length': 'Road length [meters]',
    'vkt_per_hour': 'Vehicle Kilometers Traveled per hour [vehicles * km / hour]',
    'surface': "Road surface type ['asphalt', 'paving_stones', 'compacted', None, 'unpaved', 'sett', "
       "'paved', 'cobblestone', 'metal', 'ground', 'gravel', 'dirt',"
       "'concrete:plates']",
    'avg_traffic_level': 'Average traffic level [km / hour]'
}

vehicle_count_df_weekly = pd.DataFrame(
    vehicle_count_df_weekly,
    # geometry='geometry',
    # crs=daily_group_geom_crs
)

vehicle_count_df_weekly


from trafficdata.core.trafficmeasurement import get_aggregate_weekday_hourly_vehicle_count

vehicle_count_df_weekly_final = get_aggregate_weekday_hourly_vehicle_count(
    vehicle_count_df_weekly,
    maxmin=True
)


import pandas as pd

saob = pd.read_csv('./inputs/analysisoutputVKT/São Bonifácio/vehicle_count_df_weekly_final.csv')
saob



for i in range(17):
    
    cidade = pd.read_csv('./inputs/analysisoutputVKT/Florianópolis/vehicle_count_df_weekly_final.csv')
    flori_day = florianopolis.groupby("weekday")["vkt_per_hour"].sum().reset_index(name="vkt_dia")
    flori_day['vkt_dia'].sum() * 52 / 1000


import os
import pandas

cidades = [d for d in os.listdir('./inputs/analysisoutputVKT') if os.path.isdir(os.path.join('./inputs/analysisoutputVKT', d))]
resultados = []

for cidade in cidades:
    mun = pd.read_csv(f'./inputs/analysisoutputVKT/{cidade}/vehicle_count_df_weekly_final.csv')
    vkt_dia = mun.groupby("weekday")["vkt_per_hour"].sum().reset_index(name="vkt_dia")
    vkt_anual = vkt_dia['vkt_dia'].sum() * 52 / 1000
    resultados.append({"cidade": cidade, "VKT_anual": vkt_anual})

vktGrandeFLori = pd.DataFrame(resultados)

vktGrandeFLori.to_csv('./outputs/vkt_anual_traffic.csv', index=False)
vktGrandeFLori


cidades


osm_id_test = vehicle_count_df_weekly_final.iloc[3000].osm_id

for i in range(6):
    vehicle_count_df_weekly_final.loc[
        (vehicle_count_df_weekly_final.osm_id == osm_id_test) & \
        (vehicle_count_df_weekly_final.weekday == i), ['hour', 'vehicle_count']].set_index('hour').plot()


daily_group_2



daily_group_2 = daily_group_2.drop(columns='datetime')

daily_group_2['road_length'] = daily_group_2.osm_id.apply(lambda x: td.get_road_length(x).slice_length) / 1000  # td.get_road_length(road_id)

daily_group_2 = daily_group_2.groupby(by='osm_id').sum().reset_index(drop=False)

daily_group_2['vkt_per_day'] = (
    daily_group_2['vehicle_count'] * daily_group_2['road_length'] * 24
)

daily_group_2


import matplotlib.pyplot as plt
import rasterio
from rasterio.plot import show

# Caminho do SRTM
SRTM = "/home/marcosperrude/Downloads/n01_w052_1arc_v3.tif"

# Abre o raster
with rasterio.open(SRTM) as src:
    fig, ax = plt.subplots(figsize=(6,6))
    
    # Plota o raster de fundo
    show(src, ax=ax, cmap='terrain')
    
    # Plota os pontos em cima
    ax.scatter(-51.060144, 0.008941, c='red', marker='o', label='Pontos')
    
    ax.legend()
    plt.show()


daily_group_2.vkt_per_day.hist()


daily_group_2.sort_values(by="vehicle_count", ascending=False).head(50)





daily_group_2.iloc[33,0]


daily_group_2.iloc[2,0]





example_value = daily_group.iloc[1,0]

daily_group.loc[daily_group.osm_id == example_value]


daily_group_2.loc[daily_group_2.osm_id == example_value]


daily_group_2['vkt_per_day'].fillna(0).sum()




daily_group_2['vkt_per_day'].astype('float64').sum()


td.count_vehicles(
    # road_id,
    grouper=Grouper(
        key='datetime',
        freq='1D'  # Daily frequency
    )
).head(50)


td.get_road_length()


# Exportando
if not Path('vehicle_cvehicle_count_daily-{start_date}_to_{end_date}.csv').exists():
    # daily_group.to_csv(f'vehicle_count_daily-{start_date}_to_{end_date}.csv')
    daily_group.to_parquet(f'vehicle_count_daily-{start_date}_to_{end_date}_rev1.parquet')
